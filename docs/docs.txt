The image processing software component of this project has two components: a) face detection and b) detection of a laser pointer. The face detection component is based on the work of Darrell et al. (1998), who outlined a relatively simple face detection algorithm. I have adjusted this algorithm to adapt to the constraints of this project. I also implement a rudimentary form of machine learning so as to improve the face detection algorithm.

The foundation of the image processing being performed is the Image class, defined in src/Image.h. The members and functions of this class are described as follows:

Members

int width, height, depth: The dimensions of the image. Depth is the number of color channels - usually 3 for red, green, and blue in that order, or 1 for grayscale images.

std::vector< std::vector< std::vector<int> > > mat: 3-dimensional vector containing the numerical values of the image. Each element is an integer between 0 and 255 inclusive. I use vectors rather than arrays due to ease of use.

std::vector< std::vector<bool> > valid: 2-dimensional vector that acts as a filter for the image, of size widthxheight. Based on various operations, each pixel can be set to a boolean value based on this vector.

Functions

Constructors and destructors: The usual.

void colorFilter(int r, int g, int b, double tolerance): If the color of a pixel is not sufficiently close (defined by the tolerance parameter) to the value defined by (r,g,b), its filter is set to false. The metric for closeness is defined logarithmically, based on the method outlined by Darrell et al. (1998).

void erode(int r): Erosion is an image morphological operation. For each pixel, if not all pixels within an area of rxr are true, that pixel is set to false. 

void dilate(int r): Dilation is an image morphological operation. For every true pixel, set all pixels within an area of rxr to true.

std::vector<int> largestConnComp(Image& img): Finds the bounding box for the largest connected component of true pixels. Returns a vector in the form [smallest x value, smallest y value, width, height]. A connected component is one in which every true value is connected to at least one other true value. First, this function finds the size of the largest connected component and keeps track of one pixel in that component. Then, the bounds on that connected component are found. Both of these steps are accomplished by the flood-fill algorithm. Finally, all the data within those bounds is written to the img parameter, passed by reference.

void subtractColor(int r, int g, int b): Subtracts an rgb value from all pixels.

std::vector<int> averageColor(std::vector<int> bounds, bool val): Average color of image within some bounds. Returns vector in format [average r, average g, average b].

void flatten(): Converts to grayscale by averaging r, g, b. 

void threshhold(int thresh): Threshholds a grayscale image. All pixels with a value less than thresh have their filters set to false.

void scaleDown(int w, int h): Scales an image down to size wxh. Uses a basic averaging algorithm.

int hammingDist(Image img): Computes sum of absolute differences between each pixel in two images.

std::vector<int> detectFace(Image& standard, int r, int g, int b, int& loss, std::vector<int>& params, const double IMG_LEARNING_RATE): Overall function which detects faces. More explanation below:

Face Detection Algorithm

First, the image is filtered based on similarity to skin color. Then, an opening morphological operation is performed (this consists of erosion followed by dilation). The purpose of this is to eliminate noise, such as very small connected components or irregular edges. The largest connected component is then found. A face, if one exists in the image, is likely to be located here. The image is then cropped to only where this connected component is located. The average face color is subtracted from the image, and it is threshholded and converted to grayscale. This produces a mask of the face. It is then scaled down to 16x16, to be compared to a standard mask of the face. If the hamming distance between the two images is sufficiently small, the face is detected. 

Laser Detection Algorithm

This algorithm is original and considerably simpler than the face detection algorithm. The image is converted to grayscale and then threshholded. Typically, as our webcam produces rather dark images, the laser will be the only object sufficiently bright. However, if other bright objects are in the image, some checks are implemented to see if the detected area is really a laser. If the area is small enough and the most common color is red (we are using a red laser), it is considered a laser. Otherwise, we repeat this process with a different area, until a laser is found or all areas are processed.

Machine Learning

While the Omega2 is not powerful enough to support true machine learning, I have implemented a very rudimentary form of machine learning where the algorithm can continuously self-improve as it makes more face detections. If a face is detected in the image, the average color of the face is found. A weighted average of these values and the original "standard" face color is done, which becomes the new face color to be used in filtering the next image. A similar operation is done on the mask of the face. The previous mask is updated with a weighted average of the new values. At the end, the updated mask and color are written to a file, so these values can be reused when the program is next run. This allows the face detection algorithm to continuously improve itself, without the need for external input - a basic form of unsupervised machine learning. 
