Testing

Hardware and software components were first tested separately, then together. For software, most testing focused on the facial and laser pointer detection algorithms. Images with and without faces or laser pointers were taken in different lighting conditions and with images of different sizes. Using the OpenCV library, intermediate steps in each detection algorithm were displayed, using a separate ImageTest.cpp file which is not part of the project source code. This allowed for refinement of the parameters used in the algorithm, such as skin tone and threshhold. These tests were done on a laptop instead of the Omega2, so it was easy to display and compare results. 

Hardware testing was mostly performed on the RGB LED. An LEDtest.sh shell script was created to write arbitrary RGB values to the LED. Using this script, we calibrated the raw RGB values to more closely resemble human perception. In particular, we strengthened the red and blue signal and weakened the green signal, as well as strengthening the brightest color. 

After each part was found to work separately, we migrated the source code to the Omega2 and tested the project as a whole. Again, we experimented with various lighting conditions and angles. We also adjusted the image size so it could be processed in a reasonable time by the Omega2 and not run out of memory. We first used bright, primary colors in the background to make it obvious if the LED had changed color. Once these tests were passed, we then used subtler colors. We found that while the RGB LED was not always capable of faithfully reproducing the original color (probably due to limitations of the LED itself), it was consistently able to output the most dominant color in the background. We also tested various corner cases to asses the logging capabilities of the project. 
